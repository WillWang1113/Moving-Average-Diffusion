{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999996\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "root_pth = \"/home/user/data/THU-timeseries\"\n",
    "# root_pth = \"/mnt/ExtraDisk/wcx/research/THU-timeseries\"\n",
    "all_data_pth = [\n",
    "    # \"dataset/MFRED_clean.csv\",\n",
    "    # \"ETT-small/ETTh1.csv\",\n",
    "    # \"ETT-small/ETTh2.csv\",\n",
    "    # \"ETT-small/ETTm1.csv\",\n",
    "    # \"ETT-small/ETTm2.csv\",\n",
    "    # 'exchange_rate/exchange_rate.csv',\n",
    "    # 'illness/national_illness.csv',\n",
    "    # 'weather/weather.csv',\n",
    "    # 'traffic/traffic.csv',\n",
    "    'electricity/electricity.csv',\n",
    "]\n",
    "for data_pth in all_data_pth:\n",
    "    df = pd.read_csv(os.path.join(root_pth, data_pth), index_col=0, parse_dates=True)\n",
    "    n_timestep = len(df)\n",
    "    num_train = int(n_timestep * 0.7)\n",
    "    df = np.lib.stride_tricks.sliding_window_view(df['OT'].values.flatten(), 96*2)\n",
    "    df = (df - df.mean(axis=1, keepdims=True))/df.std(axis=1, keepdims=True)\n",
    "    print(df.std())\n",
    "    \n",
    "    # sns.jointplot(df, x=0, y=1)\n",
    "        \n",
    "    # scaler = StandardScaler()\n",
    "    # scaler.fit(df[:num_train])\n",
    "    # df[df.columns] = scaler.transform(df)\n",
    "    # df.index = pd.date_range('2019-01-01', periods=len(df), freq='5min')\n",
    "        \n",
    "    # df.to_csv(os.path.join(root_pth, 'MFRED_scaled.csv'))\n",
    "    # long_df = []\n",
    "    # for c in df.columns.tolist():\n",
    "    #     temp_df = df[[c]].copy()\n",
    "    #     temp_df['unique_id'] = c\n",
    "    #     temp_df = temp_df.reset_index()\n",
    "    #     temp_df = temp_df.rename(columns={c:'y','date':'ds'})\n",
    "    #     if data_pth.__contains__('ETTh'):\n",
    "    #         num_train = 12 * 30 * 24\n",
    "    #         num_test = 4 * 30 * 24\n",
    "    #         num_vali = 4 * 30 * 24\n",
    "    #         temp_df = temp_df[:num_train+num_vali+num_test]\n",
    "    #     elif data_pth.__contains__('ETTm'):\n",
    "    #         num_train = 12 * 30 * 24 * 4\n",
    "    #         num_test = 4 * 30 * 24 * 4\n",
    "    #         num_vali = 4 * 30 * 24 * 4\n",
    "    #         temp_df = temp_df[:num_train+num_vali+num_test]\n",
    "    #     long_df.append(temp_df)\n",
    "    # long_df = pd.concat(long_df)\n",
    "    # long_df = long_df.reset_index(drop=True)\n",
    "    # long_df.to_csv(os.path.join(root_pth, data_pth[:-4]+'_NF.csv'), index=False)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasetsforecast.long_horizon import LongHorizon, LongHorizonInfo\n",
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import (\n",
    "    NHITS,\n",
    "    DLinear,\n",
    "    PatchTST,\n",
    "    TimesNet,\n",
    "    # iTransformer,\n",
    "    # TFT,\n",
    "    Autoformer,\n",
    "    # FEDformer,\n",
    "    # LSTM,\n",
    "    # MLP,\n",
    "    # NBEATSx,\n",
    "    # DeepAR,\n",
    ")\n",
    "from neuralforecast.losses.numpy import mse, mae\n",
    "from neuralforecast.losses.pytorch import MQLoss, DistributionLoss\n",
    "from neuralforecast.losses.numpy import mqloss, mse, mae\n",
    "from src.utils.metrics import calculate_metrics, get_bench_metrics\n",
    "import pickle\n",
    "\n",
    "# Change this to your own data to try the model\n",
    "print(LongHorizonInfo['ECL'].test_size)\n",
    "Y_df, _, _ = LongHorizon.load(directory='/home/user/data/NF_longterm', group='ECL')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "\n",
    "# For this excercise we are going to take 960 timestamps as validation and test\n",
    "n_time = len(Y_df.ds.unique())\n",
    "num_train = int(n_time * 0.7)\n",
    "num_test = int(n_time * 0.2)\n",
    "print(num_test)\n",
    "num_vali = n_time - num_train - num_test\n",
    "# num_train = 12 * 30 * 24\n",
    "# num_test = 4 * 30 * 24\n",
    "# num_vali = 4 * 30 * 24\n",
    "        \n",
    "# val_size = 96*10\n",
    "# test_size = 96*10\n",
    "\n",
    "Y_df = Y_df[Y_df['unique_id'] == 'OT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = Y_hat_df[\"y\"].values.reshape(-1, pred_len, 1)\n",
    "print(y_true.shape)\n",
    "y_true[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE METRICS\n",
    "ds = [\n",
    "    \"ETTh1\",\n",
    "    \"ETTh2\",\n",
    "    \"ETTm1\",\n",
    "    \"ETTm2\",\n",
    "    \"ECL\",\n",
    "    \"Exchange\",\n",
    "    \"TrafficL\",\n",
    "    \"Weather\",\n",
    "]\n",
    "pred_len = [96, 192, 336, 720]\n",
    "save_dir = \"/mnt/ExtraDisk/wcx/research/benchmarks\"\n",
    "all_df = []\n",
    "for d in ds:\n",
    "    ds_df = []\n",
    "    for pl in pred_len:\n",
    "        result_path = os.path.join(save_dir, f\"{d}_96_{pl}_U\",\"results.csv\")\n",
    "        df = pd.read_csv(result_path, index_col=0)\n",
    "        df.index.name = 'model'\n",
    "        df = df.reset_index()\n",
    "        df = df.groupby('model').mean()\n",
    "        df = df.drop(columns=['MAE', 'iter'])\n",
    "        df = df.stack()\n",
    "        df = pd.DataFrame(df, columns=[pl]).transpose()\n",
    "        ds_df.append(df)\n",
    "    ds_df = pd.concat(ds_df)\n",
    "    ds_df.index.name = 'pred_len'\n",
    "    ds_df['dataset'] = d\n",
    "    ds_df = ds_df.reset_index()\n",
    "    ds_df = ds_df.set_index(['dataset','pred_len'])\n",
    "    all_df.append(ds_df)\n",
    "all_df = pd.concat(all_df)\n",
    "all_df.to_csv(os.path.join(save_dir, 'bench_result.csv'))\n",
    "# print(all_df.to_latex(float_format=\"{:.3f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasetsforecast.long_horizon import LongHorizon\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ds = {\n",
    "        \"ETTh1\": \"ETTh1\",\n",
    "        \"ETTh2\": \"ETTh2\",\n",
    "        \"ETTm1\": \"ETTm1\",\n",
    "        \"ETTm2\": \"ETTm2\",\n",
    "        \"ECL\": \"electricity\",\n",
    "        \"Exchange\": \"exchange_rate\",\n",
    "        \"TrafficL\": \"traffic\",\n",
    "        \"Weather\": \"weather\",\n",
    "        \"MFRED\": \"mfred\",\n",
    "    }\n",
    "\n",
    "for d in ds:\n",
    "    print(d)\n",
    "    if d=='Weather':\n",
    "        data_dir_nf = '/mnt/ExtraDisk/wcx/research'\n",
    "        dataset_nf = d\n",
    "        data_dir_thu = '/mnt/ExtraDisk/wcx/research/THU-timeseries'\n",
    "        dataset_thu = f\"{'ETT-small' if ds[d].__contains__('ETT') else ds[d]}/{ds[d]}.csv\"\n",
    "        csv_thu = os.path.join(data_dir_thu, dataset_thu)\n",
    "        \n",
    "        # THU\n",
    "        df_thu = pd.read_csv(csv_thu, index_col=0, parse_dates=True)[['OT']]\n",
    "        if d.__contains__(\"ETTh\"):\n",
    "            num_train = 12 * 30 * 24\n",
    "            # num_test = 4 * 30 * 24\n",
    "            # num_vali = 4 * 30 * 24\n",
    "        elif d.__contains__(\"ETTm\"):\n",
    "            num_train = 12 * 30 * 24 * 4\n",
    "            # num_test = 4 * 30 * 24 * 4\n",
    "            # num_vali = 4 * 30 * 24 * 4\n",
    "        else:\n",
    "            num_train = int(len(df_thu) * 0.7)\n",
    "            # num_test = int(len(Y_df) * 0.2)\n",
    "            # num_vali = len(Y_df) - num_train - num_test\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df_thu.values[:num_train])\n",
    "        df_thu_value = scaler.transform(df_thu.values)\n",
    "        \n",
    "            \n",
    "        # NF\n",
    "        Y_df, _, _ = LongHorizon.load(directory=data_dir_nf, group=dataset_nf)\n",
    "        Y_df[\"ds\"] = pd.to_datetime(Y_df[\"ds\"])\n",
    "        df_nf = Y_df[Y_df[\"unique_id\"] == \"OT\"]\n",
    "        df_nf = df_nf.drop(columns=['unique_id'])\n",
    "        df_nf = df_nf.set_index('ds')\n",
    "        \n",
    "        if df_thu_value.shape != df_nf.values.shape:\n",
    "            print(\"shape is not the same\")\n",
    "            print(f'{d}')\n",
    "            print(df_thu_value.shape)\n",
    "            print(df_nf.values.shape)\n",
    "            max_len = min(len(df_thu_value), len(df_nf.values))\n",
    "        else:\n",
    "            max_len = len(df_thu_value)\n",
    "        \n",
    "        close = np.allclose(df_thu_value.flatten()[:max_len], df_nf.values.flatten()[:max_len])\n",
    "        close = np.allclose(df_thu_value.flatten()[-10000:], df_nf.values.flatten()[-10000:])\n",
    "        print(close)\n",
    "        if not close:\n",
    "            # plt.plot(df_thu_value.flatten()[-10000:])\n",
    "            # plt.plot(df_nf.values.flatten()[-10000:])\n",
    "            plt.scatter(df_thu_value.flatten()[-10000:], df_nf.values.flatten()[-10000:])\n",
    "        print('----'*10)\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.scatter(df_thu_value.flatten()[:max_len], df_nf.values.flatten()[:max_len])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasetsforecast.long_horizon import LongHorizon\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from neuralforecast.losses.numpy import mse, mqloss\n",
    "\n",
    "# # SAVE METRICS\n",
    "ds = [\n",
    "    \"ETTh1\",\n",
    "    \"ETTh2\",\n",
    "    \"ETTm1\",\n",
    "    \"ETTm2\",\n",
    "    \"ECL\",\n",
    "    \"Exchange\",\n",
    "    \"TrafficL\",\n",
    "    \"Weather\",\n",
    "]\n",
    "quantiles = [0.05 * i for i in range(19)]\n",
    "# pred_len = [336]\n",
    "pred_len = [96, 192, 336, 720]\n",
    "save_dir = \"/mnt/ExtraDisk/wcx/research/benchmarks\"\n",
    "all_df = []\n",
    "for d in ds:\n",
    "    # if d != 'TrafficL':\n",
    "    #     continue\n",
    "    ds_df = []\n",
    "    for pl in pred_len:\n",
    "        label_path = os.path.join(save_dir, f\"{d}_96_{pl}_U\", \"true.npy\")\n",
    "        pred_path = os.path.join(save_dir, f\"{d}_96_{pl}_U\", \"pred_PatchTST_0.npy\")\n",
    "        y_true = np.load(label_path)\n",
    "        y_pred = np.load(pred_path)\n",
    "        y_true_fft = np.fft.rfft(y_true[0].flatten(), norm=\"ortho\")\n",
    "        y_true_fft[0] = 0\n",
    "        y_tr_fft_abs = np.abs(y_true_fft)\n",
    "        theta = np.arctan2(y_true_fft.imag, y_true_fft.real)\n",
    "        phi_r_int = np.where(\n",
    "            np.isinf(y_tr_fft_abs),\n",
    "            np.ones_like(y_tr_fft_abs),\n",
    "            ((y_tr_fft_abs - 1) / (y_tr_fft_abs + 1)),\n",
    "        )\n",
    "        phi = np.arcsin(phi_r_int)\n",
    "        # print(theta[0])\n",
    "        print(theta[-1])\n",
    "        # y_true = y_true[::pl].flatten()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(theta)\n",
    "        # ax.plot(y_pred[333, ..., 9].flatten())\n",
    "\n",
    "        ax.set_title(d)\n",
    "        # plt.plot(y_true[126].flatten())\n",
    "        # plt.plot(y_pred[126, ..., 9].flatten())\n",
    "        break\n",
    "    # break\n",
    "#         df.index.name = 'model'\n",
    "#         df = df.reset_index()\n",
    "#         df = df.groupby('model').mean()\n",
    "#         df = df.drop(columns=['MAE', 'iter'])\n",
    "#         df = df.stack()\n",
    "#         df = pd.DataFrame(df, columns=[pl]).transpose()\n",
    "#         ds_df.append(df)\n",
    "#     ds_df = pd.concat(ds_df)\n",
    "#     ds_df.index.name = 'pred_len'\n",
    "#     ds_df['dataset'] = d\n",
    "#     ds_df = ds_df.reset_index()\n",
    "#     ds_df = ds_df.set_index(['dataset','pred_len'])\n",
    "#     all_df.append(ds_df)\n",
    "# all_df = pd.concat(all_df)\n",
    "# all_df.to_csv(os.path.join(save_dir, 'bench_result.csv'))\n",
    "# print(all_df.to_latex(float_format=\"{:.3f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "\n",
    "ds = {\n",
    "    \"ETTh1\": \"etth1\",\n",
    "    # \"ETTh2\": \"etth2\",\n",
    "    # \"ETTm1\": \"ettm1\",\n",
    "    # \"ETTm2\": \"ettm2\",\n",
    "    # \"ECL\": \"electricity\",\n",
    "    # \"Exchange\": \"exchange_rate\",\n",
    "    # \"TrafficL\": \"traffic\",\n",
    "    # \"Weather\": \"weather\",\n",
    "    # \"MFRED\": \"mfred\",\n",
    "}\n",
    "# pred_len = [288, 432, 576]\n",
    "pred_len = [96]\n",
    "# pred_len = [96, 192, 336, 720]\n",
    "save_dir = \"/home/user/data/FrequencyDiffusion/savings\"\n",
    "save_dir_bench = \"/home/user/data/NF_benchmark\"\n",
    "# save_dir = \"/mnt/ExtraDisk/wcx/research/FrequencyDiffusion/savings\"\n",
    "all_df = []\n",
    "\n",
    "for d in ds:\n",
    "    real_d = ds[d]\n",
    "    ds_df = []\n",
    "    for pl in pred_len:\n",
    "        print(d, pl)\n",
    "        result_path = os.path.join(save_dir, f\"{real_d}_{pl}_S\", \"test_dl.pt\")\n",
    "        test_dl = torch.load(result_path)\n",
    "        y_real = []\n",
    "        x = []\n",
    "        for b in test_dl:\n",
    "            x.append(b[\"observed_data\"].cpu().numpy())\n",
    "            y_real.append(b[\"future_data\"].cpu().numpy())\n",
    "        y_real = np.concatenate(y_real)\n",
    "        print(y_real.shape)\n",
    "        print(y_real.flatten()[:10])\n",
    "        print(y_real.flatten()[-10:])\n",
    "        # print(y_real[-1])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    #     x_norm = (x - x.mean(axis=1, keepdims=True))/x.std(axis=1, keepdims=True)\n",
    "    #     y_real_norm = (y_real - x.mean(axis=1, keepdims=True))/x.std(axis=1, keepdims=True)\n",
    "\n",
    "    #     fig, axs = plt.subplots(3,3)\n",
    "    #     axs = axs.flatten()\n",
    "    #     choose = np.random.randint(0, len(x), size=len(axs))\n",
    "    #     for i in range(len(axs)):\n",
    "    #         # axs[i].plot(range(0, x.shape[1]), x[choose[i], :, 0])\n",
    "    #         # axs[i].plot(range(0, x.shape[1]), np.mean(x[choose[i], :, 0], axis=0, keepdims=True).repeat(x.shape[1]), ls='--', c='grey')\n",
    "    #         # axs[i].plot(range(x.shape[1], x.shape[1]+y_real.shape[1]), y_real[choose[i], :, 0])\n",
    "    #         # axs[i].plot(range(x.shape[1], x.shape[1]+y_real.shape[1]), np.mean(y_real[choose[i], :, 0], axis=0, keepdims=True).repeat(y_real.shape[1]), ls='--', c='grey')\n",
    "\n",
    "    #         axs[i].plot(range(0, x_norm.shape[1]), x_norm[choose[i], :, 0])\n",
    "    #         axs[i].plot(range(0, x_norm.shape[1]), np.mean(x_norm[choose[i], :, 0], axis=0, keepdims=True).repeat(x.shape[1]), ls='--', c='grey')\n",
    "    #         axs[i].plot(range(x.shape[1], x.shape[1]+y_real.shape[1]), y_real_norm[choose[i], :, 0])\n",
    "    #         axs[i].plot(range(x.shape[1], x.shape[1]+y_real.shape[1]), np.mean(y_real_norm[choose[i], :, 0], axis=0, keepdims=True).repeat(y_real_norm.shape[1]), ls='--', c='grey')\n",
    "    #     # fig.suptitle(f'{d}-96-{pl}')\n",
    "    #     fig.tight_layout()\n",
    "    #     break\n",
    "    # break\n",
    "    # fig.savefig(f'../assets/{d}-96-{pl}.png')\n",
    "\n",
    "    # bench_path = os.path.join(save_dir_bench, f\"{d}_96_{pl}_S\", \"true.npy\")\n",
    "    # y_real_bench = np.load(bench_path)\n",
    "    # print(np.allclose(y_real, y_real_bench))\n",
    "    # # assert np.allclose(y_real, y_real_bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../assets/bj_aq_online.csv')\n",
    "df = df.dropna(axis=1, how='all')\n",
    "for i, temp_df in df.groupby('station_id'):\n",
    "    temp_df = temp_df.drop(columns=['id', 'station_id'])\n",
    "    temp_df['time'] = pd.to_datetime(temp_df['time'])\n",
    "    temp_df = temp_df.sort_values('time')\n",
    "    temp_df = temp_df.set_index('time')\n",
    "    temp_df[:100].plot()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      "{} &     ECL &   ETTh2 &   ETTm2 &  Exchange &  traffic &  weather &     0 \\\\\n",
      "\\midrule\n",
      "Dlinear                          &  0.3926 &  0.2239 &  0.1255 &    0.3681 &   0.3226 &   0.1913 &  3.00 \\\\\n",
      "PatchTST                         &  0.4116 &  0.2026 &  0.1219 &    0.5154 &   0.1784 &   0.1889 &  2.33 \\\\\n",
      "CSDI                             &  0.4581 &  0.2571 &  2.1230 &    1.2557 &   0.4991 &   0.1938 &  5.50 \\\\\n",
      "SSSD                             &  1.0257 &  0.7201 &  0.8936 &    2.9004 &   1.9662 &   0.6905 &  7.17 \\\\\n",
      "D3VAE                            &  0.8450 &  1.3961 &  3.3449 &    2.1086 &   6.3583 &   1.5461 &  7.67 \\\\\n",
      "TMDM                             &  0.4071 &  0.2508 &  0.1789 &    0.7885 &   0.1805 &   0.2209 &  4.50 \\\\\n",
      "mr-diff                          &  0.5287 &  0.2172 &  0.1700 &    0.4801 &   0.2471 &   0.2078 &  4.17 \\\\\n",
      "best\\_MADtime\\_pl\\_FactOnly\\_FreqDoi &  0.3329 &  0.2000 &  0.1233 &    0.3569 &   0.1613 &   0.1963 &  1.67 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_461340/65969149.py:44: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(df.to_latex())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bs = 64\n",
    "# model_name = f\"MADtime_pl_Full_SETKS_FreqDoi_bs{bs}\"\n",
    "# model_name = f\"MADtime_pl_Full_SETKS_bs{bs}\"\n",
    "# model_name = f\"MADtime_pl_FactOnly_FreqDoi_bs{bs}\"\n",
    "model_name = \"best_MADtime_pl_FactOnly_FreqDoi\"\n",
    "datasets = ['ECL','ETTh2','ETTm2','Exchange','Weather']\n",
    "\n",
    "new_df = pd.read_csv(f\"../assets/{model_name}.csv\")\n",
    "# new_df = pd.read_csv('../assets/MADtime_pl_doublenorm_FreqDenoise.csv')[['MSE']]\n",
    "# print(new_df)\n",
    "new_df = new_df.rename(columns={\"MSE\": model_name})\n",
    "new_df = new_df.drop(columns=[\"CRPS\", \"method\"])\n",
    "df = pd.read_csv(\"../assets/results_MSE.csv\", delimiter=\"\\t\")\n",
    "# print(df)\n",
    "\n",
    "df = pd.merge(df, new_df)\n",
    "\n",
    "# df = pd.concat([df, new_df], axis=1)\n",
    "df = df.groupby(\"dataset\").mean()\n",
    "# # df\n",
    "# df = df[['Dlinear', 'PatchTST', 'CSDI','D3VAE', 'SSSD', 'TMDM', 'mr-diff','MADtime_freqdenoise_bs128']]\n",
    "df = df[\n",
    "    [\n",
    "        \"Dlinear\",\n",
    "        \"PatchTST\",\n",
    "        \"CSDI\",\n",
    "        \"SSSD\",\n",
    "        \"D3VAE\",\n",
    "        \"TMDM\",\n",
    "        \"mr-diff\",\n",
    "        model_name,\n",
    "    ]\n",
    "]\n",
    "# df = df[['Dlinear', 'PatchTST', 'CSDI', 'SSSD', 'D3VAE','TMDM', 'mr-diff','MADfreq_pl_doublenorm_dp0.3','MADtime_freqdenoise']]\n",
    "df = df.T\n",
    "# df = df.drop(columns=['MFRED']).round(4)\n",
    "# df = df.drop(columns=[\"MFRED\", \"ETTh1\", \"ETTm1\"]).round(4)\n",
    "df = df.round(4)\n",
    "df = pd.concat([df, df.rank().mean(axis=1).round(2)], axis=1)\n",
    "df\n",
    "\n",
    "print(df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ECL</th>\n",
       "      <th>ETTh1</th>\n",
       "      <th>ETTh2</th>\n",
       "      <th>ETTm1</th>\n",
       "      <th>ETTm2</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CSDI</th>\n",
       "      <td>0.1939</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.1638</td>\n",
       "      <td>0.1156</td>\n",
       "      <td>0.4720</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>4.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSSD</th>\n",
       "      <td>0.3216</td>\n",
       "      <td>0.4509</td>\n",
       "      <td>0.3108</td>\n",
       "      <td>0.4808</td>\n",
       "      <td>0.3565</td>\n",
       "      <td>0.6743</td>\n",
       "      <td>6.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3VAE</th>\n",
       "      <td>0.3111</td>\n",
       "      <td>0.1919</td>\n",
       "      <td>0.4173</td>\n",
       "      <td>0.4831</td>\n",
       "      <td>0.6497</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TMDM</th>\n",
       "      <td>0.1881</td>\n",
       "      <td>0.1212</td>\n",
       "      <td>0.1591</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.1253</td>\n",
       "      <td>0.2959</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mr-diff</th>\n",
       "      <td>0.2357</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>0.1647</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>0.1293</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MADtime_pl_doublenorm</th>\n",
       "      <td>0.1841</td>\n",
       "      <td>0.1049</td>\n",
       "      <td>0.1547</td>\n",
       "      <td>0.0778</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.2025</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MADtime_pl_doublenorm_FreqDenoise</th>\n",
       "      <td>0.1859</td>\n",
       "      <td>0.1069</td>\n",
       "      <td>0.1556</td>\n",
       "      <td>0.0776</td>\n",
       "      <td>0.1108</td>\n",
       "      <td>0.2031</td>\n",
       "      <td>2.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ECL   ETTh1   ETTh2   ETTm1   ETTm2  \\\n",
       "CSDI                               0.1939  0.1450  0.1638  0.1156  0.4720   \n",
       "SSSD                               0.3216  0.4509  0.3108  0.4808  0.3565   \n",
       "D3VAE                              0.3111  0.1919  0.4173  0.4831  0.6497   \n",
       "TMDM                               0.1881  0.1212  0.1591  0.1143  0.1253   \n",
       "mr-diff                            0.2357  0.1032  0.1647  0.0767  0.1293   \n",
       "MADtime_pl_doublenorm              0.1841  0.1049  0.1547  0.0778  0.1099   \n",
       "MADtime_pl_doublenorm_FreqDenoise  0.1859  0.1069  0.1556  0.0776  0.1108   \n",
       "\n",
       "                                   Exchange     0  \n",
       "CSDI                                 0.3028  4.83  \n",
       "SSSD                                 0.6743  6.33  \n",
       "D3VAE                                0.5380  6.50  \n",
       "TMDM                                 0.2959  3.50  \n",
       "mr-diff                              0.2117  3.17  \n",
       "MADtime_pl_doublenorm                0.2025  1.50  \n",
       "MADtime_pl_doublenorm_FreqDenoise    0.2031  2.17  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bs = 128\n",
    "# model_name = f\"MADtime_pl_Full_SETKS_FreqDoi_bs{bs}\"\n",
    "model_name = f\"MADtime_pl_Full_SETKS_bs{bs}\"\n",
    "# model_name = f\"MADtime_pl_FactOnly_FreqDoi_bs{bs}\"\n",
    "# model_name = \"best_MADtime_pl_FactOnly_FreqDoi\"\n",
    "model_name = \"MADtime_pl_doublenorm_FreqDenoise\"\n",
    "\n",
    "\n",
    "new_df = pd.read_csv(f\"../assets/{model_name}.csv\")\n",
    "# new_df = pd.read_csv('../assets/.csv')[['MSE']]\n",
    "# print(new_df)\n",
    "new_df = new_df.rename(columns={\"CRPS\": model_name})\n",
    "new_df = new_df.drop(columns=[\"MSE\", \"method\"])\n",
    "df = pd.read_csv(\"../assets/results_CRPS.csv\", delimiter=\"\\t\")\n",
    "# print(df)\n",
    "\n",
    "df = pd.merge(df, new_df)\n",
    "\n",
    "# df = pd.concat([df, new_df], axis=1)\n",
    "df = df.groupby(\"dataset\").mean()\n",
    "df = df[\n",
    "    [\n",
    "        \"CSDI\",\n",
    "        \"SSSD\",\n",
    "        \"D3VAE\",\n",
    "        \"TMDM\",\n",
    "        \"mr-diff\",\n",
    "        \"MADtime_pl_doublenorm\",\n",
    "        model_name,\n",
    "    ]\n",
    "]\n",
    "# df = df[['Dlinear', 'PatchTST', 'CSDI', 'SSSD', 'D3VAE','TMDM', 'mr-diff','MADfreq_pl_doublenorm_dp0.3','MADtime_freqdenoise']]\n",
    "df = df.T\n",
    "# df = df.drop(columns=['MFRED']).round(4)\n",
    "# df = df.drop(columns=[\"MFRED\", \"ETTh1\", \"ETTm1\"]).round(4)\n",
    "df = df.round(4)\n",
    "df = pd.concat([df, df.rank().mean(axis=1).round(2)], axis=1)\n",
    "df\n",
    "\n",
    "# print(df.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d50082ca3605edaedbc537b9ac34a583897f1c451fa7fce6362ff0a321b45ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
