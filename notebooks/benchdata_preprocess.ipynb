{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "root_pth = \"/home/user/data/THU-timeseries\"\n",
    "# root_pth = \"/mnt/ExtraDisk/wcx/research/THU-timeseries\"\n",
    "all_data_pth = [\n",
    "    # \"dataset/MFRED_clean.csv\",\n",
    "    # \"ETT-small/ETTh1.csv\",\n",
    "    # \"ETT-small/ETTh2.csv\",\n",
    "    # \"ETT-small/ETTm1.csv\",\n",
    "    # \"ETT-small/ETTm2.csv\",\n",
    "    # 'exchange_rate/exchange_rate.csv',\n",
    "    # 'illness/national_illness.csv',\n",
    "    'weather/weather.csv',\n",
    "    # 'traffic/traffic.csv',\n",
    "    # 'electricity/electricity.csv',\n",
    "]\n",
    "for data_pth in all_data_pth:\n",
    "    df = pd.read_csv(os.path.join(root_pth, data_pth), index_col=0, parse_dates=True)\n",
    "    n_timestep = len(df)\n",
    "    num_train = int(n_timestep * 0.7)\n",
    "    df['rh (%)'][:3840].plot()\n",
    "    break\n",
    "    # scaler = StandardScaler()\n",
    "    # scaler.fit(df[:num_train])\n",
    "    # df[df.columns] = scaler.transform(df)\n",
    "    # df.index = pd.date_range('2019-01-01', periods=len(df), freq='5min')\n",
    "        \n",
    "    # df.to_csv(os.path.join(root_pth, 'MFRED_scaled.csv'))\n",
    "    # long_df = []\n",
    "    # for c in df.columns.tolist():\n",
    "    #     temp_df = df[[c]].copy()\n",
    "    #     temp_df['unique_id'] = c\n",
    "    #     temp_df = temp_df.reset_index()\n",
    "    #     temp_df = temp_df.rename(columns={c:'y','date':'ds'})\n",
    "    #     if data_pth.__contains__('ETTh'):\n",
    "    #         num_train = 12 * 30 * 24\n",
    "    #         num_test = 4 * 30 * 24\n",
    "    #         num_vali = 4 * 30 * 24\n",
    "    #         temp_df = temp_df[:num_train+num_vali+num_test]\n",
    "    #     elif data_pth.__contains__('ETTm'):\n",
    "    #         num_train = 12 * 30 * 24 * 4\n",
    "    #         num_test = 4 * 30 * 24 * 4\n",
    "    #         num_vali = 4 * 30 * 24 * 4\n",
    "    #         temp_df = temp_df[:num_train+num_vali+num_test]\n",
    "    #     long_df.append(temp_df)\n",
    "    # long_df = pd.concat(long_df)\n",
    "    # long_df = long_df.reset_index(drop=True)\n",
    "    # long_df.to_csv(os.path.join(root_pth, data_pth[:-4]+'_NF.csv'), index=False)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasetsforecast.long_horizon import LongHorizon, LongHorizonInfo\n",
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import (\n",
    "    NHITS,\n",
    "    DLinear,\n",
    "    PatchTST,\n",
    "    TimesNet,\n",
    "    # iTransformer,\n",
    "    # TFT,\n",
    "    Autoformer,\n",
    "    # FEDformer,\n",
    "    # LSTM,\n",
    "    # MLP,\n",
    "    # NBEATSx,\n",
    "    # DeepAR,\n",
    ")\n",
    "from neuralforecast.losses.numpy import mse, mae\n",
    "from neuralforecast.losses.pytorch import MQLoss, DistributionLoss\n",
    "from neuralforecast.losses.numpy import mqloss, mse, mae\n",
    "from src.utils.metrics import calculate_metrics, get_bench_metrics\n",
    "import pickle\n",
    "\n",
    "# Change this to your own data to try the model\n",
    "print(LongHorizonInfo['ECL'].test_size)\n",
    "Y_df, _, _ = LongHorizon.load(directory='/home/user/data/NF_longterm', group='ECL')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "\n",
    "# For this excercise we are going to take 960 timestamps as validation and test\n",
    "n_time = len(Y_df.ds.unique())\n",
    "num_train = int(n_time * 0.7)\n",
    "num_test = int(n_time * 0.2)\n",
    "print(num_test)\n",
    "num_vali = n_time - num_train - num_test\n",
    "# num_train = 12 * 30 * 24\n",
    "# num_test = 4 * 30 * 24\n",
    "# num_vali = 4 * 30 * 24\n",
    "        \n",
    "# val_size = 96*10\n",
    "# test_size = 96*10\n",
    "\n",
    "Y_df = Y_df[Y_df['unique_id'] == 'OT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = Y_hat_df[\"y\"].values.reshape(-1, pred_len, 1)\n",
    "print(y_true.shape)\n",
    "y_true[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE METRICS\n",
    "ds = [\n",
    "    \"ETTh1\",\n",
    "    \"ETTh2\",\n",
    "    \"ETTm1\",\n",
    "    \"ETTm2\",\n",
    "    \"ECL\",\n",
    "    \"Exchange\",\n",
    "    \"TrafficL\",\n",
    "    \"Weather\",\n",
    "]\n",
    "pred_len = [96, 192, 336, 720]\n",
    "save_dir = \"/mnt/ExtraDisk/wcx/research/benchmarks\"\n",
    "all_df = []\n",
    "for d in ds:\n",
    "    ds_df = []\n",
    "    for pl in pred_len:\n",
    "        result_path = os.path.join(save_dir, f\"{d}_96_{pl}_U\",\"results.csv\")\n",
    "        df = pd.read_csv(result_path, index_col=0)\n",
    "        df.index.name = 'model'\n",
    "        df = df.reset_index()\n",
    "        df = df.groupby('model').mean()\n",
    "        df = df.drop(columns=['MAE', 'iter'])\n",
    "        df = df.stack()\n",
    "        df = pd.DataFrame(df, columns=[pl]).transpose()\n",
    "        ds_df.append(df)\n",
    "    ds_df = pd.concat(ds_df)\n",
    "    ds_df.index.name = 'pred_len'\n",
    "    ds_df['dataset'] = d\n",
    "    ds_df = ds_df.reset_index()\n",
    "    ds_df = ds_df.set_index(['dataset','pred_len'])\n",
    "    all_df.append(ds_df)\n",
    "all_df = pd.concat(all_df)\n",
    "all_df.to_csv(os.path.join(save_dir, 'bench_result.csv'))\n",
    "# print(all_df.to_latex(float_format=\"{:.3f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasetsforecast.long_horizon import LongHorizon\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ds = {\n",
    "        \"ETTh1\": \"ETTh1\",\n",
    "        \"ETTh2\": \"ETTh2\",\n",
    "        \"ETTm1\": \"ETTm1\",\n",
    "        \"ETTm2\": \"ETTm2\",\n",
    "        \"ECL\": \"electricity\",\n",
    "        \"Exchange\": \"exchange_rate\",\n",
    "        \"TrafficL\": \"traffic\",\n",
    "        \"Weather\": \"weather\",\n",
    "        \"MFRED\": \"mfred\",\n",
    "    }\n",
    "\n",
    "for d in ds:\n",
    "    print(d)\n",
    "    if d=='Weather':\n",
    "        data_dir_nf = '/mnt/ExtraDisk/wcx/research'\n",
    "        dataset_nf = d\n",
    "        data_dir_thu = '/mnt/ExtraDisk/wcx/research/THU-timeseries'\n",
    "        dataset_thu = f\"{'ETT-small' if ds[d].__contains__('ETT') else ds[d]}/{ds[d]}.csv\"\n",
    "        csv_thu = os.path.join(data_dir_thu, dataset_thu)\n",
    "        \n",
    "        # THU\n",
    "        df_thu = pd.read_csv(csv_thu, index_col=0, parse_dates=True)[['OT']]\n",
    "        if d.__contains__(\"ETTh\"):\n",
    "            num_train = 12 * 30 * 24\n",
    "            # num_test = 4 * 30 * 24\n",
    "            # num_vali = 4 * 30 * 24\n",
    "        elif d.__contains__(\"ETTm\"):\n",
    "            num_train = 12 * 30 * 24 * 4\n",
    "            # num_test = 4 * 30 * 24 * 4\n",
    "            # num_vali = 4 * 30 * 24 * 4\n",
    "        else:\n",
    "            num_train = int(len(df_thu) * 0.7)\n",
    "            # num_test = int(len(Y_df) * 0.2)\n",
    "            # num_vali = len(Y_df) - num_train - num_test\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df_thu.values[:num_train])\n",
    "        df_thu_value = scaler.transform(df_thu.values)\n",
    "        \n",
    "            \n",
    "        # NF\n",
    "        Y_df, _, _ = LongHorizon.load(directory=data_dir_nf, group=dataset_nf)\n",
    "        Y_df[\"ds\"] = pd.to_datetime(Y_df[\"ds\"])\n",
    "        df_nf = Y_df[Y_df[\"unique_id\"] == \"OT\"]\n",
    "        df_nf = df_nf.drop(columns=['unique_id'])\n",
    "        df_nf = df_nf.set_index('ds')\n",
    "        \n",
    "        if df_thu_value.shape != df_nf.values.shape:\n",
    "            print(\"shape is not the same\")\n",
    "            print(f'{d}')\n",
    "            print(df_thu_value.shape)\n",
    "            print(df_nf.values.shape)\n",
    "            max_len = min(len(df_thu_value), len(df_nf.values))\n",
    "        else:\n",
    "            max_len = len(df_thu_value)\n",
    "        \n",
    "        close = np.allclose(df_thu_value.flatten()[:max_len], df_nf.values.flatten()[:max_len])\n",
    "        close = np.allclose(df_thu_value.flatten()[-10000:], df_nf.values.flatten()[-10000:])\n",
    "        print(close)\n",
    "        if not close:\n",
    "            # plt.plot(df_thu_value.flatten()[-10000:])\n",
    "            # plt.plot(df_nf.values.flatten()[-10000:])\n",
    "            plt.scatter(df_thu_value.flatten()[-10000:], df_nf.values.flatten()[-10000:])\n",
    "        print('----'*10)\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.scatter(df_thu_value.flatten()[:max_len], df_nf.values.flatten()[:max_len])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasetsforecast.long_horizon import LongHorizon\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from neuralforecast.losses.numpy import mse, mqloss\n",
    "\n",
    "# # SAVE METRICS\n",
    "ds = [\n",
    "    \"ETTh1\",\n",
    "    \"ETTh2\",\n",
    "    \"ETTm1\",\n",
    "    \"ETTm2\",\n",
    "    \"ECL\",\n",
    "    \"Exchange\",\n",
    "    \"TrafficL\",\n",
    "    \"Weather\",\n",
    "]\n",
    "quantiles = [0.05 * i for i in range(19)]\n",
    "# pred_len = [336]\n",
    "pred_len = [96, 192, 336, 720]\n",
    "save_dir = \"/mnt/ExtraDisk/wcx/research/benchmarks\"\n",
    "all_df = []\n",
    "for d in ds:\n",
    "    # if d != 'TrafficL':\n",
    "    #     continue\n",
    "    ds_df = []\n",
    "    for pl in pred_len:\n",
    "        label_path = os.path.join(save_dir, f\"{d}_96_{pl}_U\", \"true.npy\")\n",
    "        pred_path = os.path.join(save_dir, f\"{d}_96_{pl}_U\", \"pred_PatchTST_0.npy\")\n",
    "        y_true = np.load(label_path)\n",
    "        y_pred = np.load(pred_path)\n",
    "        y_true_fft = np.fft.rfft(y_true[0].flatten(), norm=\"ortho\")\n",
    "        y_true_fft[0] = 0\n",
    "        y_tr_fft_abs = np.abs(y_true_fft)\n",
    "        theta = np.arctan2(y_true_fft.imag, y_true_fft.real)\n",
    "        phi_r_int = np.where(\n",
    "            np.isinf(y_tr_fft_abs),\n",
    "            np.ones_like(y_tr_fft_abs),\n",
    "            ((y_tr_fft_abs - 1) / (y_tr_fft_abs + 1)),\n",
    "        )\n",
    "        phi = np.arcsin(phi_r_int)\n",
    "        # print(theta[0])\n",
    "        print(theta[-1])\n",
    "        # y_true = y_true[::pl].flatten()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(theta)\n",
    "        # ax.plot(y_pred[333, ..., 9].flatten())\n",
    "\n",
    "        ax.set_title(d)\n",
    "        # plt.plot(y_true[126].flatten())\n",
    "        # plt.plot(y_pred[126, ..., 9].flatten())\n",
    "        break\n",
    "    # break\n",
    "#         df.index.name = 'model'\n",
    "#         df = df.reset_index()\n",
    "#         df = df.groupby('model').mean()\n",
    "#         df = df.drop(columns=['MAE', 'iter'])\n",
    "#         df = df.stack()\n",
    "#         df = pd.DataFrame(df, columns=[pl]).transpose()\n",
    "#         ds_df.append(df)\n",
    "#     ds_df = pd.concat(ds_df)\n",
    "#     ds_df.index.name = 'pred_len'\n",
    "#     ds_df['dataset'] = d\n",
    "#     ds_df = ds_df.reset_index()\n",
    "#     ds_df = ds_df.set_index(['dataset','pred_len'])\n",
    "#     all_df.append(ds_df)\n",
    "# all_df = pd.concat(all_df)\n",
    "# all_df.to_csv(os.path.join(save_dir, 'bench_result.csv'))\n",
    "# print(all_df.to_latex(float_format=\"{:.3f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETTh1 96\n",
      "(2785, 96, 1)\n",
      "[-0.8623407  -0.86996883 -0.83161    -0.8775971  -0.915956   -0.8929624\n",
      " -0.9005906  -0.9005906  -0.8393472  -0.8393472 ]\n",
      "[-1.8665372 -1.7975565 -1.8665372 -1.8665372 -1.8665372 -1.8665372\n",
      " -1.8665372 -1.6595954 -1.6288646 -1.6136082]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "\n",
    "ds = {\n",
    "    \"ETTh1\": \"etth1\",\n",
    "    # \"ETTh2\": \"etth2\",\n",
    "    # \"ETTm1\": \"ettm1\",\n",
    "    # \"ETTm2\": \"ettm2\",\n",
    "    # \"ECL\": \"electricity\",\n",
    "    # \"Exchange\": \"exchange_rate\",\n",
    "    # \"TrafficL\": \"traffic\",\n",
    "    # \"Weather\": \"weather\",\n",
    "    # \"MFRED\": \"mfred\",\n",
    "}\n",
    "# pred_len = [288, 432, 576]\n",
    "pred_len = [96]\n",
    "# pred_len = [96, 192, 336, 720]\n",
    "save_dir = \"/home/user/data/FrequencyDiffusion/savings\"\n",
    "save_dir_bench = \"/home/user/data/NF_benchmark\"\n",
    "# save_dir = \"/mnt/ExtraDisk/wcx/research/FrequencyDiffusion/savings\"\n",
    "all_df = []\n",
    "\n",
    "for d in ds:\n",
    "    real_d = ds[d]\n",
    "    ds_df = []\n",
    "    for pl in pred_len:\n",
    "        print(d, pl)\n",
    "        result_path = os.path.join(save_dir, f\"{real_d}_{pl}_S\", \"test_dl.pt\")\n",
    "        test_dl = torch.load(result_path)\n",
    "        y_real = []\n",
    "        x = []\n",
    "        for b in test_dl:\n",
    "            x.append(b[\"observed_data\"].cpu().numpy())\n",
    "            y_real.append(b[\"future_data\"].cpu().numpy())\n",
    "        y_real = np.concatenate(y_real)\n",
    "        print(y_real.shape)\n",
    "        print(y_real.flatten()[:10])\n",
    "        print(y_real.flatten()[-10:])\n",
    "        # print(y_real[-1])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    #     x_norm = (x - x.mean(axis=1, keepdims=True))/x.std(axis=1, keepdims=True)\n",
    "    #     y_real_norm = (y_real - x.mean(axis=1, keepdims=True))/x.std(axis=1, keepdims=True)\n",
    "\n",
    "    #     fig, axs = plt.subplots(3,3)\n",
    "    #     axs = axs.flatten()\n",
    "    #     choose = np.random.randint(0, len(x), size=len(axs))\n",
    "    #     for i in range(len(axs)):\n",
    "    #         # axs[i].plot(range(0, x.shape[1]), x[choose[i], :, 0])\n",
    "    #         # axs[i].plot(range(0, x.shape[1]), np.mean(x[choose[i], :, 0], axis=0, keepdims=True).repeat(x.shape[1]), ls='--', c='grey')\n",
    "    #         # axs[i].plot(range(x.shape[1], x.shape[1]+y_real.shape[1]), y_real[choose[i], :, 0])\n",
    "    #         # axs[i].plot(range(x.shape[1], x.shape[1]+y_real.shape[1]), np.mean(y_real[choose[i], :, 0], axis=0, keepdims=True).repeat(y_real.shape[1]), ls='--', c='grey')\n",
    "\n",
    "    #         axs[i].plot(range(0, x_norm.shape[1]), x_norm[choose[i], :, 0])\n",
    "    #         axs[i].plot(range(0, x_norm.shape[1]), np.mean(x_norm[choose[i], :, 0], axis=0, keepdims=True).repeat(x.shape[1]), ls='--', c='grey')\n",
    "    #         axs[i].plot(range(x.shape[1], x.shape[1]+y_real.shape[1]), y_real_norm[choose[i], :, 0])\n",
    "    #         axs[i].plot(range(x.shape[1], x.shape[1]+y_real.shape[1]), np.mean(y_real_norm[choose[i], :, 0], axis=0, keepdims=True).repeat(y_real_norm.shape[1]), ls='--', c='grey')\n",
    "    #     # fig.suptitle(f'{d}-96-{pl}')\n",
    "    #     fig.tight_layout()\n",
    "    #     break\n",
    "    # break\n",
    "    # fig.savefig(f'../assets/{d}-96-{pl}.png')\n",
    "\n",
    "    # bench_path = os.path.join(save_dir_bench, f\"{d}_96_{pl}_S\", \"true.npy\")\n",
    "    # y_real_bench = np.load(bench_path)\n",
    "    # print(np.allclose(y_real, y_real_bench))\n",
    "    # # assert np.allclose(y_real, y_real_bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d50082ca3605edaedbc537b9ac34a583897f1c451fa7fce6362ff0a321b45ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
