{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "root_pth = \"/home/user/data/THU-timeseries\"\n",
    "all_data_pth = [\n",
    "    \"MFRED/MFRED.csv\",\n",
    "    # \"ETT-small/ETTh1.csv\",\n",
    "    # \"ETT-small/ETTh2.csv\",\n",
    "    # \"ETT-small/ETTm1.csv\",\n",
    "    # \"ETT-small/ETTm2.csv\",\n",
    "    # 'exchange_rate/exchange_rate.csv',\n",
    "    # 'illness/national_illness.csv',\n",
    "    # 'weather/weather.csv',\n",
    "]\n",
    "for data_pth in all_data_pth:\n",
    "    df = pd.read_csv(os.path.join(root_pth, data_pth), index_col=0, parse_dates=True)\n",
    "    long_df = []\n",
    "    for c in df.columns.tolist():\n",
    "        temp_df = df[[c]].copy()\n",
    "        temp_df['unique_id'] = c\n",
    "        temp_df = temp_df.reset_index()\n",
    "        temp_df = temp_df.rename(columns={c:'y','date':'ds'})\n",
    "        if data_pth.__contains__('ETTh'):\n",
    "            num_train = 12 * 30 * 24\n",
    "            num_test = 4 * 30 * 24\n",
    "            num_vali = 4 * 30 * 24\n",
    "            temp_df = temp_df[:num_train+num_vali+num_test]\n",
    "        elif data_pth.__contains__('ETTm'):\n",
    "            num_train = 12 * 30 * 24 * 4\n",
    "            num_test = 4 * 30 * 24 * 4\n",
    "            num_vali = 4 * 30 * 24 * 4\n",
    "            temp_df = temp_df[:num_train+num_vali+num_test]\n",
    "        long_df.append(temp_df)\n",
    "    long_df = pd.concat(long_df)\n",
    "    long_df = long_df.reset_index(drop=True)\n",
    "    long_df.to_csv(os.path.join(root_pth, data_pth[:-4]+'_NF.csv'), index=False)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.61717033],\n",
       "       [-0.78310746],\n",
       "       [-0.90936404],\n",
       "       [-1.1366258 ],\n",
       "       [-1.205165  ],\n",
       "       [-1.210576  ],\n",
       "       [-1.3260106 ],\n",
       "       [-1.4721075 ],\n",
       "       [-1.63083   ],\n",
       "       [-1.4522672 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pth = \"/home/user/data/Benchmarks/metric_results/long_term_forecast_ETTh1_96_192_DLinear_ETTh1_ftS_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0/true.npy\"\n",
    "pth = \"/home/user/data/Benchmarks/metric_results/long_term_forecast_electricity_96_192_DLinear_custom_ftS_sl96_ll48_pl192_dm512_nh8_el2_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0/true.npy\"\n",
    "y_true = np.load(pth)\n",
    "y_true[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5260\n",
      "5260\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasetsforecast.long_horizon import LongHorizon, LongHorizonInfo\n",
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import (\n",
    "    NHITS,\n",
    "    DLinear,\n",
    "    PatchTST,\n",
    "    TimesNet,\n",
    "    # iTransformer,\n",
    "    # TFT,\n",
    "    Autoformer,\n",
    "    # FEDformer,\n",
    "    # LSTM,\n",
    "    # MLP,\n",
    "    # NBEATSx,\n",
    "    # DeepAR,\n",
    ")\n",
    "from neuralforecast.losses.numpy import mse, mae\n",
    "from neuralforecast.losses.pytorch import MQLoss, DistributionLoss\n",
    "from neuralforecast.losses.numpy import mqloss, mse, mae\n",
    "from src.utils.metrics import calculate_metrics, get_bench_metrics\n",
    "import pickle\n",
    "\n",
    "# Change this to your own data to try the model\n",
    "print(LongHorizonInfo['ECL'].test_size)\n",
    "Y_df, _, _ = LongHorizon.load(directory='/home/user/data/NF_longterm', group='ECL')\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "\n",
    "# For this excercise we are going to take 960 timestamps as validation and test\n",
    "n_time = len(Y_df.ds.unique())\n",
    "num_train = int(n_time * 0.7)\n",
    "num_test = int(n_time * 0.2)\n",
    "print(num_test)\n",
    "num_vali = n_time - num_train - num_test\n",
    "# num_train = 12 * 30 * 24\n",
    "# num_test = 4 * 30 * 24\n",
    "# num_vali = 4 * 30 * 24\n",
    "        \n",
    "# val_size = 96*10\n",
    "# test_size = 96*10\n",
    "\n",
    "Y_df = Y_df[Y_df['unique_id'] == 'OT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Seed set to 1\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | loss          | MQLoss        | 19    \n",
      "1 | padder_train  | ConstantPad1d | 0     \n",
      "2 | scaler        | TemporalNorm  | 0     \n",
      "3 | decomp        | SeriesDecomp  | 0     \n",
      "4 | linear_trend  | Linear        | 353 K \n",
      "5 | linear_season | Linear        | 353 K \n",
      "------------------------------------------------\n",
      "707 K     Trainable params\n",
      "19        Non-trainable params\n",
      "707 K     Total params\n",
      "2.831     Total estimated model params size (MB)\n",
      "/home/user/anaconda3/envs/freqdiff310/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:104: Total length of `TimeSeriesLoader` across ranks is zero. Please make sure this was your intention.\n",
      "/home/user/anaconda3/envs/freqdiff310/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:104: Total length of `CombinedLoader` across ranks is zero. Please make sure this was your intention.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: No training batches.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(val_check_interval=1)` was configured so validation will run after every batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/freqdiff310/lib/python3.10/site-packages/neuralforecast/core.py:199: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "level = [i for i in range(10, 100, 10)]\n",
    "quantiles = [0.05 * (1 + i) for i in range(19)]\n",
    "seq_len = 96\n",
    "pred_len = 192\n",
    "config = {\n",
    "    \"h\": pred_len,\n",
    "    \"input_size\": seq_len,\n",
    "    \"max_steps\": 100,\n",
    "    \"loss\": MQLoss(level=level),\n",
    "    \"early_stop_patience_steps\": 5,\n",
    "    \"val_check_steps\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"valid_batch_size\":32,\n",
    "    \"inference_windows_batch_size\": 32,\n",
    "    \"windows_batch_size\": 32,\n",
    "    \"scaler_type\": \"standard\",\n",
    "    'devices':1,\n",
    "    \"fast_dev_run\": False,\n",
    "    'drop_last_loader': True,\n",
    "    \"num_sanity_val_steps\": 0,\n",
    "    \"random_seed\": 1,\n",
    "    \"enable_progress_bar\": True,\n",
    "    \"accelerator\":'cpu',\n",
    "    \"default_root_dir\": \"/home/user/data/FrequencyDiffusion/savings/mfred/benchmarks\",\n",
    "}\n",
    "\n",
    "models = [\n",
    "    # TSMixer(**config, n_series=n_series, loss=MQLoss(level=level)),\n",
    "    # PatchTST(**config),\n",
    "    # TimesNet(**config),\n",
    "    DLinear(**config),\n",
    "    # NHITS(**config),\n",
    "    # Autoformer(**config)\n",
    "    # FEDformer(**config, loss=MQLoss(level=level)),\n",
    "    # DeepAR(\n",
    "    #     **config,\n",
    "    #     loss=DistributionLoss(distribution=\"Normal\", num_samples=200, level=level),\n",
    "    # ),\n",
    "]\n",
    "nf = NeuralForecast(models=models, freq=\"5min\")\n",
    "\n",
    "\n",
    "Y_hat_df = nf.cross_validation(\n",
    "    df=Y_df,\n",
    "    val_size=num_vali,\n",
    "    test_size=num_test,\n",
    "    n_windows=None,\n",
    ")\n",
    "# cols = Y_hat_df.columns.tolist()\n",
    "# print(Y_hat_df)\n",
    "# y_true = Y_hat_df[\"y\"].values.reshape(-1, pred_len, n_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5069, 192, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.61717031],\n",
       "       [-0.78310749],\n",
       "       [-0.90936403],\n",
       "       [-1.13662581],\n",
       "       [-1.20516508],\n",
       "       [-1.21057607],\n",
       "       [-1.32601063],\n",
       "       [-1.47210749],\n",
       "       [-1.63083   ],\n",
       "       [-1.45226717]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = Y_hat_df[\"y\"].values.reshape(-1, pred_len, 1)\n",
    "print(y_true.shape)\n",
    "y_true[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "freqdiff310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
